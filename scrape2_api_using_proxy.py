import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import random
import time

proxies = proxies = [
    "99.79.124.70:80",
    "35.86.81.136:3128",
    "18.132.36.51:3128",
    "34.221.119.219:999",
    "52.11.48.124:3128",
    "13.40.152.64:3128",
    "18.236.65.56:3129",
    "13.40.3.184:3128",
    "52.194.186.70:1080",
    "54.245.34.166:10001",
    "159.69.57.20:8880",
    "172.188.122.92:80",
    "47.254.88.250:13001",
    "102.23.245.112:8080",
    "128.140.113.110:3128",
    "45.170.226.250:999",
    "148.206.32.3:8080",
    "79.116.15.191:3128",
    "47.238.149.53:8888",
    "67.43.228.253:31571",
    "35.90.245.227:31293",
    "18.236.175.208:10001",
    "106.117.208.101:7890",
    "119.156.195.171:3128",
    "14.241.80.37:8080",
    "27.79.191.139:16000",
    "72.10.164.178:11517",
    "34.143.143.61:7777",
    "52.56.248.120:10001",
    "8.209.255.13:3128",
    "47.243.92.199:3128",
    "47.89.184.18:3128",
    "20.210.39.153:8561",
    "103.82.132.248:8888",
    "8.215.3.250:3128",
    "51.17.101.31:20202",
    "72.10.160.174:15711",
    "64.181.240.152:3128",
    "67.43.228.254:16535",
    "80.190.82.58:14602",
    "47.250.177.202:8443",
    "157.230.247.60:1488",
    "18.170.63.85:10001",
    "47.91.65.23:3128",
    "8.213.151.128:3128",
    "47.91.104.88:3128",
    "66.201.7.237:3128",
    "54.214.109.103:10001",
    "27.79.148.169:16000",
    "50.235.247.114:8085",
    "114.80.37.192:3081",
    "36.64.37.18:89",
    "36.136.27.2:4999",
    "111.1.61.47:3128",
    "47.122.65.32:8081",
    "47.122.61.139:1081",
    "47.92.93.226:8888",
    "103.121.165.229:1443",
    "47.122.56.158:7777",
    "221.202.27.194:10811",
    "201.77.96.0:999",
    "18.190.207.123:3128",
    "104.247.51.75:3128",
    "147.75.34.92:443",
    "195.199.240.179:8888",
    "203.74.125.18:8888",
    "156.251.62.4:8118",
    "188.166.197.129:3128",
    "72.240.9.63:80",
    "44.220.205.79:8080",
    "193.39.9.147:8181",
    "116.108.120.66:4003",
    "27.79.140.6:16000",
    "178.255.148.228:8118",
    "18.135.101.146:3128",
    "115.77.139.197:10009",
    "158.69.118.135:38080",
    "123.20.59.55:12058",
    "35.177.23.165:8000",
    "89.58.38.239:3128",
    "34.48.171.130:33080",
    "45.129.141.49:3128",
    "116.110.95.147:9002",
    "14.190.69.99:12001",
    "42.113.144.196:5675",
    "141.145.213.155:3128",
    "35.153.237.105:8080",
    "164.92.74.154:8118",
    "27.79.177.153:16000",
    "65.108.203.37:28080",
    "194.163.147.233:14602",
    "27.79.236.87:16000",
    "54.37.72.89:80",
    "65.109.61.30:60008",
    "118.113.247.103:2324",
    "93.113.63.73:33100",
    "43.167.162.60:13001",
    "43.167.190.20:13001",
    "43.167.185.139:13001",
    "43.167.167.192:13001"
]


proxies_list = [{"http": f"http://{ip}", "https": f"http://{ip}"} for ip in proxies]
random.shuffle(proxies_list)

base_url = "https://www.flipkart.com/search?q=samsung+mobile&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={}"

pages_to_scrape = 3
ua = UserAgent()

for proxy in proxies_list:
    try:
        user_agent = ua.random
        if "mobile" in user_agent.lower() or "iphone" in user_agent.lower() or "android" in user_agent.lower():
            user_agent = ua.chrome

        headers = {"User-Agent": user_agent}
        print(f"\nüïµÔ∏è Trying proxy: {proxy['http']} with UA: {user_agent}")

        for page in range(1, pages_to_scrape + 1):
            url = base_url.format(page)
            response = requests.get(url, headers=headers, proxies=proxy, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")


            items = soup.find_all("div", class_="Nx9bqj _4b5DiR")

            if not items:
                print(f"‚ö†Ô∏è No product items found on page {page}. Possibly blocked or structure changed.")
                continue

            print(f"\nüìÑ Page {page} Results:")
            for item in items:
                print(item.get_text(strip=True))

            time.sleep(random.uniform(2, 4))

        print("\n‚úÖ Scraping complete with this proxy.")
        break  # Exit on first successful scrape

    except Exception as e:
        print(f"‚ùå Proxy failed: {e}")
